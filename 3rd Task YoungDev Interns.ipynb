{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63ee82df",
   "metadata": {},
   "source": [
    "# Deep Learning for Image Classification:\n",
    "## * Build and fine-tune convolutional neural networks (CNNs) for image classification tasks ## * using frameworks like TensorFlow o r PyTorch. Explore advanced architectures like transfer learning with pre-trained models and conduct hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e7aa4e3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13536\\3881584321.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 10\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "\n",
    "# CIFAR-10 dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Load pre-trained ResNet50 model\n",
    "model = models.resnet50(pretrained=True)\n",
    "num_ftrs = model.fc.in_features\n",
    "\n",
    "# Replace the last fully connected layer for transfer learning\n",
    "model.fc = nn.Linear(num_ftrs, 10)  # 10 classes in CIFAR-10 dataset\n",
    "\n",
    "# Send the model to GPU if available\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Fine-tuning the model\n",
    "total_steps = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{total_steps}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Accuracy of the model on the test images: {100 * correct / total}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ac3d1f",
   "metadata": {},
   "source": [
    "# Natural Language Processing (NLP) Projects:\n",
    "## * Develop advanced NLP models for tasks like machine translation, text summarization, or language generation.\n",
    "## * Work with transformer-based models like BERT or GPT-3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edd916ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13536\\940258291.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGPT2LMHeadModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGPT2Tokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Load pre-trained GPT-2 model and tokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'gpt2-medium'\u001b[0m  \u001b[1;31m# You can choose different sizes of GPT-2: 'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "model_name = 'gpt2-medium'  # You can choose different sizes of GPT-2: 'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "# Set seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Generate text based on a given prompt\n",
    "def generate_text(prompt, max_length=100, temperature=0.7):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    output = model.generate(input_ids, max_length=max_length, temperature=temperature, num_return_sequences=1)\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "# Example prompt\n",
    "prompt = \"Once upon a time, in a land far far away\"\n",
    "generated_text = generate_text(prompt, max_length=150, temperature=0.7)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d46481f",
   "metadata": {},
   "source": [
    "# Time Series Forecasting at Scale:\n",
    "## * Build large-scale time series forecasting models for industries like finance, energy, or demand forecasting.\n",
    "## * Implement distributed computing with technologies like Dask or Apache Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e35c9502",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3099231369.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_13536\\3099231369.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    pip install dask[complete]\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pip install dask[complete]\n",
    "pip install pystan==2.19.1.1\n",
    "pip install fbprophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78283d93",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fbprophet'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13536\\1691730347.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mfbprophet\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mProphet\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataframe\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mdd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistributed\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mClient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLocalCluster\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'fbprophet'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fbprophet import Prophet\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "# Create a Dask cluster\n",
    "cluster = LocalCluster()\n",
    "client = Client(cluster)\n",
    "\n",
    "# Generate synthetic time series data\n",
    "def generate_time_series(start_date, end_date, freq='D'):\n",
    "    dates = pd.date_range(start=start_date, end=end_date, freq=freq)\n",
    "    values = np.random.randn(len(dates))\n",
    "    return pd.DataFrame({'ds': dates, 'y': values})\n",
    "\n",
    "# Define function for parallel forecasting\n",
    "def forecast_prophet(df):\n",
    "    prophet = Prophet()\n",
    "    prophet.fit(df)\n",
    "    future = prophet.make_future_dataframe(periods=30)  # Forecasting 30 days ahead\n",
    "    forecast = prophet.predict(future)\n",
    "    return forecast[['ds', 'yhat']]\n",
    "\n",
    "# Generate synthetic time series data (you can replace this with your own data)\n",
    "start_date = '2024-01-01'\n",
    "end_date = '2024-12-31'\n",
    "time_series_data = generate_time_series(start_date, end_date)\n",
    "\n",
    "# Convert the pandas DataFrame to a Dask DataFrame\n",
    "dask_df = dd.from_pandas(time_series_data, npartitions=4)\n",
    "\n",
    "# Perform parallel forecasting using Dask\n",
    "forecast_df = dask_df.groupby('division').apply(forecast_prophet, meta={'ds': 'datetime64[ns]', 'yhat': 'float'})\n",
    "\n",
    "# Compute the results\n",
    "forecast_df = forecast_df.compute()\n",
    "\n",
    "# Shutdown the Dask client and cluster\n",
    "client.close()\n",
    "cluster.close()\n",
    "\n",
    "# Print the forecasted values\n",
    "print(forecast_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86139dd3",
   "metadata": {},
   "source": [
    "# Recommendation Systems:\n",
    "## * Create personalized recommendation systems using collaborative filtering or content-based approaches.\n",
    "## * Explore matrix factorization or deep learning techniques for recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a625a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-surprise\n",
      "  Using cached scikit-surprise-1.1.3.tar.gz (771 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'error'\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  ERROR: Error [WinError 225] Operation did not complete successfully because the file contains a virus or potentially unwanted software while executing command python setup.py egg_info\n",
      "ERROR: Could not install packages due to an OSError: [WinError 225] Operation did not complete successfully because the file contains a virus or potentially unwanted software\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-surprise\n",
    "from surprise import Dataset, Reader\n",
    "from surprise import SVD\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise import accuracy\n",
    "\n",
    "# Load the MovieLens dataset (replace file_path with your actual file path)\n",
    "file_path = 'path/to/ml-100k/u.data'\n",
    "reader = Reader(line_format='user item rating timestamp', sep='\\t')\n",
    "data = Dataset.load_from_file(file_path, reader)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "trainset, testset = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the SVD algorithm\n",
    "model = SVD()\n",
    "model.fit(trainset)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = model.test(testset)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy.rmse(predictions)\n",
    "\n",
    "# Generate recommendations for a given user\n",
    "user_id = str(196)  # Replace with any user ID\n",
    "user_ratings = []\n",
    "for movie_id in range(1, 1683):  # Total number of movies in the dataset\n",
    "    user_ratings.append((user_id, str(movie_id), 4.0))  # Assume a rating of 4.0 for unrated movies\n",
    "user_predictions = model.test(user_ratings)\n",
    "recommended_movies = sorted(user_predictions, key=lambda x: x.est, reverse=True)[:10]  # Get top 10 recommendations\n",
    "\n",
    "# Print recommended movies\n",
    "print(\"Top 10 recommended movies for user\", user_id)\n",
    "for movie in recommended_movies:\n",
    "    print(movie)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46b6f13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
